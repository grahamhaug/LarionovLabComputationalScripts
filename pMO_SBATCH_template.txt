#!/bin/bash
# How many nodes to use
#SBATCH -N 1 -n 64
#SBATCH -o output.%j # Output file name (%j expands to jobID)
#SBATCH -e error.%j # Error file name (%j expands to jobid)

#SBATCH -p normal # The normal queue - use for most
##SBATCH -p skx-normal # use with 128gb for large basis sets

# How long to run the job
#SBATCH -t ${HOURS}:00:00 # Run time (hh:mm:ss)

# Select the allocation to use by uncommenting
##SBATCH -A Mechanistic-Studies
##SBATCH -A Kinetics-of-selectiv
##SBATCH -A Computational-Analys
##SBATCH -A Excited-state-reacti
#SBATCH -A SOCI-Xmers

### POINT TO SCRIPTS DIRECTORY ###
#source my aliuses/alii
shopt -s expand_aliases
#You will need to point this as where your scripts are
source /home1/05793/rdg758/scripts/alii.env

# Grab the Input job name; assign to "job"
export job=$(echo $SLURM_JOB_NAME | sed 's/-mo//')

#SCRATCH environment variable is set to new scratch directory
tdir=$(mktemp -d $SCRATCH/${job}-mo_$SLURM_JOBID-XXXX)

###   COPY RELEVANT FILES TO $SCRATCH   ###	
cp $SLURM_SUBMIT_DIR/${job}.log $tdir/

#.fchk file
cp $SLURM_SUBMIT_DIR/${job}.fchk $tdir/

#script
cp $SLURM_SUBMIT_DIR/${job} $tdir/


###   MOVE TO $SCRATCH   ###
cd $tdir

# ensure that gaussian is loaded:
module load gaussian

# timestamps are helpful
echo "job started on $(date)"

####### Cubegen data ######
${CUBES}

###   COPY POST-JOB DATA TO $WORK ###
cp $tdir/${job}*.cube $SLURM_SUBMIT_DIR

###   CLEAN $SCRATCH   ###
#check if $tdir is set
if [[ -n "$tdir" ]] && [[ -d "$tdir" ]]; then
        rm -rf "$tdir"
fi
